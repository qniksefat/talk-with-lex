2023 -
I was the Sr. Director of AI at Tesla, where I led the computer vision team of Tesla Autopilot. This includes in-house data labeling, neural network training, the science of making it work, and deployment in production running on our custom inference chip. Today, the Autopilot increases the safety and convenience of driving, but the team's goal is to develop and deploy Full Self-Driving to our rapidly growing fleet of millions of cars. Our Aug 2021 Tesla AI Day provides the most detailed and up-to-date overview of this effort.
2011 - 2015
Along the way I squeezed in 3 internships at (a baby) Google Brain in 2011 working on learning-scale unsupervised learning from videos, then again in Google Research in 2013 working on large-scale supervised learning on YouTube videos, and finally at DeepMind in 2015 working on the deep reinforcement learning team.
2005 - 2009
State of GPT @ Microsoft Build 2023 (slides)
AI for Full Self-Driving @ CVPR 2021
Multi-Task Learning in the Wilderness @ ICML 2019
2017 REâ€¢WORK Summit with Nathan Benaich
2016 Bay Area Deep Learning School: CNNs
NVIDIA GTC Keynote 2015 with Jensen Huang
my 2016 lecture videos
r/cs231n
Mar 2021 Short Story on AI: Forward Pass
Nov 2017 Software 2.0
May 2015 The Unreasonable Effectiveness of Recurrent Neural Networks
pet projects
arxiv-sanity tames the overwhelming flood of papers on Arxiv. It allows researchers to discover relevant papers, search/sort by similarity, see recent/popular papers, and get recommendations. Deployed live at arxiv-sanity.com. My obsession with meta research involved many more projects over the years, e.g. see pretty NIPS 2020 papers, research lei, scholaroctopus, and biomed-sanity. Update: my most revent arxiv-sanity-lite from-scratch rewrite is much better.
ConvNetJS is a deep learning library written from scratch entirely in Javascript. This enables nice web-based demos that train convolutional neural networks (or ordinary ones) entirely in the browser. Many web demos included. I did an interview with Data Science Weekly about the library and some of its back story here. Also see my later followups such as tSNEJS, REINFORCEjs, or recurrentjs, GANs in JS.
publications
Tianlin (Tim) Shi, Andrej Karpathy, Linxi (Jim) Fan, Jonathan Hernandez, Percy Liang
Tim Salimans, Andrej Karpathy, Xi Chen, Diederik P. Kingma, and Yaroslav Bulatov
Andrej Karpathy
Justin Johnson*, Andrej Karpathy*, Li Fei-Fei
Andrej Karpathy*, Justin Johnson*, Li Fei-Fei
Andrej Karpathy, Li Fei-Fei
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, Li Fei-Fei
Andrej Karpathy, Armand Joulin, Li Fei-Fei
Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, Li Fei-Fei
Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, Andrew Y. Ng
Andrej Karpathy, Stephen Miller, Li Fei-Fei
Adam Coates, Andrej Karpathy, Andrew Ng
Andrej Karpathy, Michiel van de Panne
Stelian Coros, Andrej Karpathy, Benjamin Jones, Lionel Reveret, Michiel van de Panne
Neural Networks: Zero To Hero lecture series
My primary blog and my other blog
I joined the 2020 edition of MIT Technology Review's 35 Innovators Under 35
Loss function Tumblr :D! My collection of funny loss functions.
MIT Technology Review amusing article from a long time ago.
A long time ago I was really into Rubik's Cubes. I learned to solve them in about 17 seconds and then, frustrated by lack of learning resources, created YouTube videos explaining the Speedcubing methods. These went on to become relatively popular. There's also my long dead cubing page. Oh, and a video of me at a Rubik's cube competition :)
